Quantum decoherence is the loss of quantum coherence. In quantum mechanics, particles such as electrons are described by a wave function, a mathematical representation of the quantum state of a system; a probabilistic interpretation of the wave function is used to explain various quantum effects. As long as there exists a definite phase relation between different states, the system is said to be coherent. A definite phase relationship is necessary to perform quantum computing on quantum information encoded in quantum states. Coherence is preserved under the laws of quantum physics.
If a quantum system were perfectly isolated, it would maintain coherence indefinitely, but it would be impossible to manipulate or investigate it. If it is not perfectly isolated, for example during a measurement, coherence is shared with the environment and appears to be lost with time; a process called quantum decoherence. As a result of this process, quantum behavior is apparently lost, just as energy appears to be lost by friction in classical mechanics.
Decoherence was first introduced in 1970 by the German physicist H. Dieter Zeh and has been a subject of active research since the 1980s. Decoherence has been developed into a complete framework, but it does not solve the measurement problem, as the founders of decoherence theory admit in their seminal papers. Decoherence can be viewed as the loss of information from a system into the environment (often modeled as a heat bath), since every system is loosely coupled with the energetic state of its surroundings.  Viewed in isolation, the system's dynamics are non-unitary (although the combined system plus environment evolves in a unitary fashion). Thus the dynamics of the system alone are irreversible. As with any coupling, entanglements are generated between the system and environment.  These have the effect of sharing quantum information with—or transferring it to—the surroundings.
Decoherence has been used to understand the possibility of the collapse of the wave function in quantum mechanics. Decoherence does not generate actual wave-function collapse.  It only provides a framework for apparent wave-function collapse, as the quantum nature of the system "leaks" into the environment.  That is, components of the wave function are decoupled from a coherent system and acquire phases from their immediate surroundings.  A total superposition of the global or universal wavefunction still exists (and remains coherent at the global level), but its ultimate fate remains an interpretational issue.  Specifically, decoherence does not attempt to explain the measurement problem.  Rather, decoherence provides an explanation for the transition of the system to a mixture of states that seem to correspond to those states observers perceive.  Moreover, our observation tells us that this mixture looks like a proper quantum ensemble in a measurement situation, as we observe that measurements lead to the "realization" of precisely one state in the "ensemble".
Decoherence represents a challenge for the practical realization of quantum computers, since such machines are expected to rely heavily on the undisturbed evolution of quantum coherences.  Simply put, they require that the coherence of states be preserved and that decoherence is managed, in order to actually perform quantum computation.  The preservation of coherence, and mitigation of decoherence effects, are thus related to the concept of quantum error correction.
To examine how decoherence operates, an "intuitive" model is presented.  The model requires some familiarity with quantum theory basics.  Analogies are made between visualisable classical phase spaces and Hilbert spaces.  A more rigorous derivation in Dirac notation shows how decoherence destroys interference effects and the "quantum nature" of systems.  Next, the density matrix approach is presented for perspective.
An N-particle system can be represented in non-relativistic quantum mechanics by a wave function ψ(x1, x2, …, xN), where each xi is a point in 3-dimensional space.  This has analogies with the classical phase space.  A classical phase space contains a real-valued function in 6N dimensions (each particle contributes 3 spatial coordinates and 3 momenta).  Our "quantum" phase space, on the other hand, involves a complex-valued function on a 3N-dimensional space.  The position and momenta are represented by operators that do not commute, and ψ lives in the mathematical structure of a Hilbert space.  Aside from these differences, however, the rough analogy holds.
Different previously isolated, non-interacting systems occupy different phase spaces. Alternatively we can say that they occupy different lower-dimensional subspaces in the phase space of the joint system.  The effective dimensionality of a system's phase space is the number of degrees of freedom present, which—in non-relativistic models—is 6 times the number of a system's free particles.  For a macroscopic system this will be a very large dimensionality.  When two systems (and the environment would be a system) start to interact, though, their associated state vectors are no longer constrained to the subspaces.  Instead the combined state vector time-evolves a path through the "larger volume", whose dimensionality is the sum of the dimensions of the two subspaces.  The extent to which two vectors interfere with each other is a measure of how "close" they are to each other (formally, their overlap or Hilbert space multiplies together) in the phase space.  When a system couples to an external environment, the dimensionality of, and hence "volume" available to, the joint state vector increases enormously. Each environmental degree of freedom contributes an extra dimension.
The original system's wave function can be expanded in many different ways as a sum of elements in a quantum superposition.  Each expansion corresponds to a projection of the wave vector onto a basis.  The basis can be chosen at will.  Let us choose an expansion where the resulting basis elements interact with the environment in an element-specific way.  Such elements will—with overwhelming probability—be rapidly separated from each other by their natural unitary time evolution along their own independent paths.  After a very short interaction, there is almost no chance of any further interference.  The process is effectively irreversible.  The different elements effectively become "lost" from each other in the expanded phase space created by coupling with the environment; in phase space, this decoupling is monitored through the Wigner quasi-probability distribution.  The original elements are said to have decohered.  The environment has effectively selected out those expansions or decompositions of the original state vector that decohere (or lose phase coherence) with each other.  This is called "environmentally-induced superselection", or einselection. The decohered elements of the system no longer exhibit quantum interference between each other, as in a double-slit experiment.  Any elements that decohere from each other via environmental interactions are said to be quantum-entangled with the environment.  The converse is not true: not all entangled states are decohered from each other.
Any measuring device or apparatus acts as an environment, since at some stage along the measuring chain, it has to be large enough to be read by humans. It must possess a very large number of hidden degrees of freedom.  In effect, the interactions may be considered to be quantum measurements. As a result of an interaction, the wave functions of the system and the measuring device become entangled with each other. Decoherence happens when different portions of the system's wave function become entangled in different ways with the measuring device. For two einselected elements of the entangled system's state to interfere, both the original system and the measuring in both elements device must significantly overlap, in the scalar product sense.  If the measuring device has many degrees of freedom, it is very unlikely for this to happen.
As a consequence, the system behaves as a classical statistical ensemble of the different elements rather than as a single coherent quantum superposition of them. From the perspective of each ensemble member's measuring device, the system appears to have irreversibly collapsed onto a state with a precise value for the measured attributes, relative to that element. And this, provided one explains how the Born rule coefficients effectively act as probabilities as per the measurement postulate, constitutes a solution to the quantum measurement problem
Depolarizing is a non-unitary transformation on a quantum system which maps pure states to mixed states. This is a non-unitary process, because any transformation that reverses this process will map states out of their respective Hilbert space thus not preserving positivity (i.e. the original probabilities are mapped to negative probabilities, which is not allowed). The 2-dimensional case of such a transformation would consist of mapping pure states on the surface of the Bloch sphere to mixed states within the Bloch sphere. This would contract the Bloch sphere by some finite amount and the reverse process would expand the Bloch sphere, which cannot happen.
Dissipation is a decohering process by which the populations of quantum states are changed due to entanglement with a bath. An example of this would be a quantum system that can exchange its energy with a bath through the interaction Hamiltonian. If the system is not in its ground state and the bath is at a temperature lower than that of the system's, then the system will give off energy to the bath, and thus higher-energy eigenstates of the system Hamiltonian will decohere to the ground state after cooling and, as such, will all be non-degenerate. Since the states are no longer degenerate, they are not distinguishable, and thus this process is irreversible (non-unitary).
Decoherence represents an extremely fast process for macroscopic objects, since these are interacting with many microscopic objects, with an enormous number of degrees of freedom, in their natural environment. The process is needed if we are to understand why we tend not to observe quantum behaviour in everyday macroscopic objects and why we do see classical fields emerge from the properties of the interaction between matter and radiation for large amounts of matter.  The time taken for off-diagonal components of the density matrix to effectively vanish is called the decoherence time. It is typically extremely short for everyday, macroscale processes. A modern basis-independent definition of the decoherence time relies on the short-time behavior of the fidelity  between the initial and the time-dependent state or, equivalently, the decay of the purity .
We assume for the moment that the system in question consists of a subsystem A being studied and the "environment" ϵ, and the total Hilbert space is the tensor product of a Hilbert space HA   describing A and a Hilbert space Hϵ describing ϵ, that is, H = HA X Hϵ.
This is a reasonably good approximation in the case where A and ϵ are relatively independent (e.g. there is nothing like parts of A mixing with parts of ϵ or conversely). The point is, the interaction with the environment is for all practical purposes unavoidable (e.g. even a single excited atom in a vacuum would emit a photon, which would then go off). 
The time it takes for U(t) (the unitary operator as a function of time) to display the decoherence property is called the decoherence time.
The decoherence rate depends on a number of factors, including temperature or uncertainty in position, and many experiments have tried to measure it depending on the external environment. The process of a quantum superposition gradually obliterated by decoherence was quantitatively measured for the first time by Serge Haroche and his co-workers at the École Normale Supérieure in Paris in 1996. Their approach involved sending individual rubidium atoms, each in a superposition of two states, through a microwave-filled cavity. The two quantum states both cause shifts in the phase of the microwave field, but by different amounts, so that the field itself is also put into a superposition of two states. Due to photon scattering on cavity-mirror imperfection, the cavity field loses phase coherence to the environment.
Haroche and his colleagues measured the resulting decoherence via correlations between the states of pairs of atoms sent through the cavity with various time delays between the atoms.
In July 2011, researchers from University of British Columbia and University of California, Santa Barbara were able to reduce environmental decoherence rate "to levels far below the threshold necessary for quantum information processing" by applying high magnetic fields in their experiment. In August 2020 scientists reported that that ionizing radiation from environmental radioactive materials and cosmic rays may substantially limit the coherence times of qubits if they aren't shielded adequately which may be critical for realizing fault-tolerant superconducting quantum computers in the future.
Criticism of the adequacy of decoherence theory to solve the measurement problem has been expressed by Anthony Leggett: "I hear people murmur the dreaded word "decoherence". But I claim that this is a major red herring". Concerning the experimental relevance of decoherence theory, Leggett has stated: "Let us now try to assess the decoherence argument. Actually, the most economical tactic at this point would be to go directly to the results of the next section, namely that it is experimentally refuted! However, it is interesting to spend a moment enquiring why it was reasonable to anticipate this in advance of the actual experiments. In fact, the argument contains several major loopholes".
Before an understanding of decoherence was developed, the Copenhagen interpretation of quantum mechanics treated wave-function collapse as a fundamental, a priori process. Decoherence as a possible explanatory mechanism for the appearance of wave function collapse was first developed by David Bohm in 1952, who applied it to Louis DeBroglie's pilot-wave theory, producing Bohmian mechanics, the first successful hidden-variables interpretation of quantum mechanics.  Decoherence was then used by Hugh Everett in 1957 to form the core of his many-worlds interpretation. However, decoherence was largely ignored for many years (with the exception of Zeh's work), and not until the 1980s did decoherent-based explanations of the appearance of wave-function collapse become popular, with the greater acceptance of the use of reduced density matrices. The range of decoherent interpretations have subsequently been extended around the idea, such as consistent histories.  Some versions of the Copenhagen interpretation have been modified to include decoherence.
Decoherence does not claim to provide a mechanism for the actual wave-function collapse; rather it puts forth a reasonable framework for the appearance of wave-function collapse. The quantum nature of the system is simply "leaked" into the environment so that a total superposition of the wave function still exists, but exists – at least for all practical purposes — beyond the realm of measurement. Of course, by definition, the claim that a merged but unmeasurable wave function still exists cannot be proven experimentally. Decoherence is needed to understand why a quantum system begins to obey classical probability rules after interacting with its environment (due to the suppression of the interference terms when applying Bohm's probability rules to the system).